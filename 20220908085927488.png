## YOLO-V3
* YOLO-V3是比较完善的一个目标检测的框架，它经过一代一代的发展，一步步地完善，接下来介绍一些YOLO-V3在V2的基础上进行了哪些的改进，下图为我训练出来模型的检测效果
<div align=center>
<img src="../../技术学习图片\43.jpg" width = "300" height = "200" alt = "检测前" align=top />
<img src="../../技术学习图片\44.jpg" width = "300" height = "200" align=top />
</div>



#### 先验框的更新
在V2中，先验框是由K-mean聚类算法得出的5种不同尺寸的先验框，在V3中考虑到Darknet-53会输出三种尺寸的预测，因此选取先验框时应该选取的是3的倍数，最终选择了9种先验框，每种尺寸都有三种不同形状的先验框。

先验框的选择仍然是通过K-mean聚类出9种，这9种聚类得出的先验框之间不是没有关系的，其中每种尺寸的特征图都有其专属的三种基本相同大小的先验框用来预测，例如：13×13尺寸的特征图中会有三种较大的先验框用来预测，26×26的特征图会有较中等的三种先验框用来预测，52×52的特征图则会有三种较小的先验框用来预测

K-mean聚类的距离度量方法仍然是利用IOU来计算，这些先验框都是根据groundtruth的数据聚类出来的
#### 正负样本分配
在前面的学习，我们已经知道除了要计算有物体的网格的损失，还要计算不含物体的网格的损失，因此就有了正负样本的说法

根据上面聚类出来的9种先验框，计算出三个尺寸所对应的三个先验框与groundtruth的IOU，具体做法为：每个尺寸用对应的三种大小的先验框去与这个尺寸的groundtruth计算IOU，如果计算出来的先验框的IOU值存在大于阈值，就标记这个先验框为这个尺寸下的正样本，并设该尺寸下IOU大于阈值的先验框的置信度为1；如果计算出来的IOU都不大于阈值，那就选取计算出来IOU最大的先验框作为正样本

以上的两种方法就是筛选正样本的方法，如果不满足这两种方法，剩下的先验框就标记为负样本。标记正负样本只会参与到置信度损失的计算中，不会参与到预测类别以及bbox位置预测的损失计算中

正负样本分配也需要遵循一些准则，那就是尽量使得正样本数和负样本数基本相等，这是为了让模型在预测时不会出现大量漏检的现象，按照我的理解就是：先验框计算出来的IOU只要大于阈值，就说明这个先验框是能检测到物体的，正样本数越多就说明这个区域内存在物体的概率更大，也就不至于出现大量漏检的现象
#### 网络结构上的改进
首先来看看它的主体网络：**Darknet-53**的结构图
<div align = center>
<img src = "../../技术学习图片\45.png" >
</div>

网络结构的改变是YOLO-V3中比较大改进的地方

在YOLO-V2中使用的主体网络结构为： **Darknet-19**，在Darknet-19中使用卷积层来提取特征图，通过池化层来获取降低了特征维度后的特征图（具体结构请去看YOLO-V2），在经过Darknet-19全部提取完特征图之后，再将上一层池化之前的结果经过**reorg**之后再合并到一起，来实现特征融合，将感受野较小的特征图融合到感受野较大的特征图上面，可以使得较少的目标不至于丢失

在YOLO-V3中的**Darknet-53**不同于V2中的网络，它除了使用卷积层来提取特征图之外，还使用了卷积层来降低特征图的维度，而不是池化层。仔细想想确实可以用卷积层来代替池化层来实现特征的降维，而且这个网络的主要成分还是残差块$\left(\text{resn}\right)$，残差块是由一层特征降维的卷积层和 $\text{2n}$ 个提取特征图的卷积层组成，但是在每 $\text{2}$ 个提取特征图的卷积层之后，都要将输出的 $\text{conv}$ 和输入的 $\text{conv}$ 相加

<div align = center>
<img src = "../../技术学习图片\46.png" >
</div>

在Darknet-53的卷积降维中会提取出大小不同的特征图，以输入图片大小为416×416为例，网络会将其降维为13×13、26×26、52×52等尺寸。若是像V2的做法，会将尺寸较大的特征图拆分之后融入尺寸较大的特征图中去，但想想V2的做法真的合适吗？将检测小物体（感受野较小）的特征图融合到检测大物体（感受野较大）的特征图中去，可能会将本来检测大物体的功能减弱或者检测小物体的功能减弱

那么V3中是怎么做的呢？它将最后提取出来的三种不同尺寸的特征图进行处理，将尺寸小的特征图进行**上采样**，还原到上一层的尺寸大小，然后将该特征图拼接到上一层的特征图中去，例如：将13×13的特征图经过上采样变为26×26大小的特征图，然后拼接到26×26尺寸的特征图中去。

> 我的理解是：在Darknet-53中通过残差块提取的特征图中已包含了前面的信息，小物体的信息也提取出来了，经过参数的优化小物体的检测会得到一定效果，此时应该向感受野大的特征图获取一些全局信息（**个人理解仍待考证**）

#### decode解码
由于网络输出的预测值的范围是0~1之间，也就是说每一个网格输出的bbox的位置预测都是都被限制在这个网格内，这正是YOLO的划分，每个网格都来预测是否有物体中心点。而decode就是用来将特征预测值都还原到相应尺寸下的尺寸，具体做法就是：将坐标预测值（0 ~ 1之间）与网格左上角的坐标相加获得该bbox在相应尺寸下的坐标，以及将预测的宽高乘上相应的尺寸，从而获得bbox的宽高，这样就可以计算出bbox与groundtruth的IOU，那么bbox的置信度就得出来了，就可以预测此处是否存在物体了
#### 处理解码后的bbox
网络输出的预测值经过解码之后，会获取所有尺寸下的网格所输出三个预测框bbox的信息，这些预测出来的bbox有可能存在左上角的坐标比右下角坐标大的情况，或者bbox不存在的情况，在检测目标时需要将这些预测出来的bbox信息去除先，保留下那些bbox信息存在且不异常的
#### NMS筛选bbox（在预测阶段）
NMS（Non-Maximum Suppression）抑制不是极大值的元素，说白了就是去除哪些重叠率较高并且score评分较低的bbox

处理了解码后的bbox，会保留下一些bbox的信息，但是不是每个bbox都去负责呢？仔细想想每个网格（对应尺寸下）会输出三个bbox，假如处理后的bbox信息都被保留下来了，那么一个物体就会有三个预测框去框住它，这样的展示效果就不太好了。

对于一个网格内的bbox，需要的是保存下那个能最好地检测出物体的那个bbox，就要用到NMS方法了
方法就是：

1. 首先判断边界框的数目是否大于0，如果不是大于0，说明没有bbox的信息被保留下来，就去检测下一个网格，直到所有尺寸下的网格都被检测完
2. 在保留bbox时，会给每个bbox一个评分，对于一个网格内的bbox，根据评分，为这些bbox排序，（不放回的取）取出评分最大的那个bbox
3. 把这个评分最高的bbox与同一网格下的其他bbox计算IOU，这是为了判断评分最大的bbox与同一网格下的其他bbox的重叠率，去除那些重叠率过高的，取到剩下的bbox为0为止
#### GIOU（广义IOU）
这是一种优化边界框的方法，GIOU是衡量bbox与实际框之间对齐和重叠情况的一个指标
IOU计算公式：
$$ \text{IOU }= \left|\frac{A \cap B}{A\cup B}\right|$$
GIOU的计算公式：
$$\text{GIOU }= \left| \frac{C/(A\cup B)}{C} \right|$$